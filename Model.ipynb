{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1ebecb52-9d0f-4559-919e-cba56bfa2ffb",
   "metadata": {},
   "source": [
    "Hi Gustavo\n",
    "Attached are 7 files. The format is changed. it now has fewer fields. For this POC, I am only looking at 2 business rules."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "614bdf52-079c-48ac-8163-0fac21194f9d",
   "metadata": {},
   "source": [
    " Here is the first rule that is reflected in the attached 11 files. It is fairly straight forward and in fact the \"input\" file already identifies rows that have discrepancy with a \"xty\" value of \"D\"! I am taking the example of some rows in the file \"part-00001-tid-7978638525664904554-70dd8fb2-21ee-44a6-98a5-19edaf12ad15-54462-1-c000.txt\". The goal is to identify all the rows (and dependent rows) for all transactions with \"xty\" value of \"D\" (Discrepancy). For ex: In the file \"part-00001-tid-7978638525664904554-70dd8fb2-21ee-44a6-98a5-19edaf12ad15-54462-1-c000.txt\", row # 1064 is \"D\". The dependent transactions are the rows with the same \"OSI\" (CHOR1), \"omb\" (49297) and \"item\" (10964). The challenge is to find the immediately preceding 2 rows (based on datetime column xdt) and 1 row that is immediately after the \"D\" rows datetime (xdt) column .If you were to put all these rows together, this is how they would look. (see attachment, image.png)\n",
    "In this image, you can see that qty_before - qty = qty_after. You don't have qty_before field in the input file. Qty_before is calculated as \"qoh\" + \"Qty\". (Only negative quantities should be summed up to arrive at qty_before. Positive quantities are when stock is put back in the cabinet)\n",
    "The first 2 rows satisfies the condition from the image (qty_before - qty = qty). The 3rd row is of type \"D\" which is the \"compensating\" transaction row. The 4th row is the \"main\" row that caused the discrepancy. The user instead of carrying forward \"1\" from 2nd row carried forward 11111. This created the compensating transaction for 11110 in row 3. Makes sense?\n",
    "Your task is to identify all these \"D\" with the preceding, compensating and main rows. I attached 7 files with 4570 rows total. Your task is to find all the anomalies like I mentioned above. The LLM must be able to group and aggregate also. If the user asks a question, how many \"anamolies\" exist within a date range, it needs to be able to answer.\n",
    "Note: Per my internal calculations, only 8 rows are qualified as \"anamolies\" (\"D\" rows). if you put preceeding 2 rows and \"main\" row, you should get a total of 24 rows. The model should be able to detect all these 24 rows and allow the user to ask questions about these anomalies in a natural language.\n",
    "I will give you the 2nd business rule tomorrow. Please revert back ASAP if you can do it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6a9a2a9-0e1d-482a-9b91-62c9b60730ec",
   "metadata": {},
   "source": [
    "2nd business rule:\r\n",
    "In a given dataset, we need to find what \"nurses\" are dispensing meds \"unusually\" outside of their peers in the same area.\r\n",
    "There are many definitions of \"unusual\" medicine dispensation. From a pure ML perspective, we can do this:\r\n",
    "We can group users per shift and area and compare the \"qty\" dispensed with other peers in the same area for the same number of \"shifts\". This is a classic ML scenario.\r\n",
    "If we were to do this, this is how we can possibly do it in classic ML.\r\n",
    "Say, we are calculating this on a weekly data we first need to find the shift gaps in users transactions per user and per area. The shift gap is usually 10 hours or more. So, for a given user \"JDoe\", if the gap between 2 sets of transactions is > 10 hours, these are done in different shifts. Let's take this sample data:\r\n",
    "User Name transaction date Area qty issued shift\r\n",
    "Jdoe 2023-07-03 11:08:51 A1 10 1\r\n",
    "Jdoe 2023-07-03 11:22:52 A1 20 1\r\n",
    "Jdoe 2023-07-03 12:23:43 A1 15 1\r\n",
    "\r\n",
    "\r\n",
    "\r\n",
    "Jdoe 2023-07-04 09:08:51 A1 5 2\r\n",
    "Jdoe 2023-07-04 10:22:52 A1 10 2\r\n",
    "Jdoe 2023-07-04 11:23:43 A1 22 2\r\n",
    "\r\n",
    "\r\n",
    "\r\n",
    "Mjane 2023-07-03 14:08:53 A1 2 1\r\n",
    "Mjane 2023-07-03 15:08:51 A1 5 1\r\n",
    "Mjane 2023-07-03 16:08:51 A1 6 1\r\n",
    "\r\n",
    "\r\n",
    "\r\n",
    "You can see from the above that, if we compare just 1 shift, the rate of dispensation from MJane is far less than Jdoe. So, Jdoe must be flagged if the user requested comparison for the date of 2023-07-03 ONLY. However, if user requested the anomalies for the week of 2027-07-03, Jdoe did 1 more shift the week of 7/3, we first have to normalize the shift count and then do a compare. (after all, we cannot compare 2 nurses when one worked for 10 shifts and one worked for 20 shifts. That's not a good comparison). So, when we compare \"qty\", we need to \"normalize\" the shift count.\r\n",
    "The above is a classic way of doing ML. We can probably do the same thing on the sample dataset.\r\n",
    "\r\n",
    "\r\n",
    "\r\n",
    "However, we want to build a intelligence to a model in such a way that, it can automatically recognize the shift patterns and the dispensed quantities in an area and present all anomalies.\r\n",
    "I will send out another set of files with some sample data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b42e9553-94c5-447e-8391-4243a8eeb03f",
   "metadata": {},
   "source": [
    "Please let me know by Monday if you can do it.\r\n",
    "Deliverables:\r\n",
    "1. A 1 page write up on the task at hand comparing methods of \"Anomaly detection\" in classic ML (using algorithms like Isolation Forest) v/s using LLM based models\r\n",
    "2. A \"Detailed explanation\" of the methods used in our model and how we arrived at the results.\r\n",
    "3. A jupyter notebook with the training code and demo\r\n",
    "4. Some \"proof\" that the results are valid.\r\n",
    "5. Deploy to production and run it\r\n",
    "6. API access to the llm so that it's callable from clients like c#, java, python etc.\r\n",
    "7. Chat interface so that we can query it using NLP\r\n",
    "8. Model has to answer \"why the discrepancy happened\" using prompt engineering.\r\n",
    "9. Solve both the business cases.\r\n",
    "10.Model has to respond to questions to \"natural language\" questions.\r\n",
    "11. This should run in google vertex AI and on any platform\r\n",
    "Ex:\r\n",
    "\r\n",
    "\r\n",
    "\r\n",
    "1. Give me all users that have anomalies in medication dispensation between Jan and feb of 2023\r\n",
    "2. Give me all unresolved discrepancies for all users for Area \"A1B\"\r\n",
    "3. Give me the rate of dispensation of medications between the users John doe and PeterParker\r\n",
    "4. Give me all unresolved discrepancies for user John doe (or for fentanyl)\r\n",
    "5. how many shifts did user JDoe work last week and how many total medications were dispensed? Also give me the anomalies in this data.\r\n",
    "5a. Give me reason for the anomalies you gave me above (prompt engineering)\r\n",
    "Bottom line: the model should do aggregations, filtering, etc. It should also give the results in tabular data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b652d2cd-9b98-4a84-af1e-a2447eaa1860",
   "metadata": {},
   "source": [
    "\n",
    "It looks like you have a comprehensive job description with two business rules that need to be addressed using a Language Model (LLM) from Hugging Face in Python. The task is to develop a Proof of Concept (POC) that can handle anomaly detection and answer natural language questions related to the anomalies found. Let's break down the job description and requirements step-by-step:\n",
    "\n",
    "Anomaly Detection for Discrepancy Transactions:\n",
    "\n",
    "The goal is to identify rows with a specific \"xty\" value of \"D\" (Discrepancy) and find their preceding, compensating, and main rows based on specific conditions.\n",
    "The model should be able to group and aggregate the identified anomalies.\n",
    "The client wants to be able to ask the model questions in natural language, such as how many anomalies exist within a date range.\n",
    "The deliverables include a 1-page write-up comparing traditional ML (using algorithms like Isolation Forest) with LLM-based models for anomaly detection.\n",
    "Anomaly Detection for Unusual Medication Dispensation:\n",
    "\n",
    "The task is to find nurses who are dispensing medications unusually outside of their peers in the same area.\n",
    "An ML approach can be used by grouping users per shift and area and comparing the \"qty\" dispensed with other peers in the same area for the same number of \"shifts.\"\n",
    "Shift gaps (usually 10 hours or more) need to be identified to determine different shifts.\n",
    "The model should be able to handle different time frames, like daily or weekly data, and normalize the shift count for meaningful comparisons.\n",
    "The deliverables include a detailed explanation of the methods used in the model, a Jupyter notebook with the training code and demo, and proof of the validity of the results.\n",
    "Model Deployment and Interface:\n",
    "\n",
    "The model needs to be deployed to production and made accessible via an API.\n",
    "The model should have a chat interface to query it using Natural Language Processing (NLP).\n",
    "The model should be able to answer questions in natural language, provide explanations for the discrepancies, and handle queries like aggregations and filtering.\n",
    "Specific Queries:\n",
    "\n",
    "The model should be able to respond to specific queries related to both business cases, such as listing users with anomalies in medication dispensation between specific dates, unresolved discrepancies for specific users or areas, rate of dispensation comparisons between users, and more.\n",
    "Data Access and Validity:\n",
    "\n",
    "The model should run on Google Vertex AI and be deployable on any platform.\n",
    "There should be evidence to demonstrate the validity of the results obtained by the model.\n",
    "To fulfill these requirements, you will need to:\n",
    "\n",
    "Train and fine-tune an LLM from Hugging Face on relevant data for both business cases.\n",
    "Implement the anomaly detection logic for both business rules, taking into account the specific conditions and comparisons required.\n",
    "Develop a chat interface for querying the model using NLP.\n",
    "Implement prompt engineering to enable the model to answer questions related to the discrepancies.\n",
    "Create a Jupyter notebook with the training code and demo to showcase the model's functionality.\n",
    "Deploy the model to production, provide API access, and demonstrate its usage for different queries.\n",
    "Keep in mind that this is a complex task that involves multiple steps and considerable development effort. Be sure to plan and prioritize your work accordingly. Good luck with your Proof of Concept development!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4123973-92e7-4686-ae89-3bcda4f159a0",
   "metadata": {},
   "source": [
    "Start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6d784c4d-35a1-4400-b730-240462df2d79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install huggingface_hub\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f8a42172-945a-4622-996b-40d0b523e37b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import requests\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "from tabulate import tabulate\n",
    "\n",
    "import chromadb\n",
    "from chromadb.config import Settings\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.llms import HuggingFaceHub\n",
    "from langchain.embeddings import HuggingFaceHubEmbeddings, HuggingFaceEmbeddings\n",
    "\n",
    "# from langchain.embeddings.huggingface import HuggingFaceEmbeddings\n",
    "from langchain.text_splitter import CharacterTextSplitter, RecursiveCharacterTextSplitter\n",
    "\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.vectorstores.base import VectorStoreRetriever\n",
    "\n",
    "\n",
    "from langchain.embeddings.sentence_transformer import SentenceTransformerEmbeddings\n",
    "\n",
    "\n",
    "import logging\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4436280d-a0e7-457d-b54b-f810189f8a8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set our Access Token for Hugging Face\n",
    "os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = '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'  # <-- Gustavo Key\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ebb4250f-00a0-4771-8191-68a428281375",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'.chromadb/'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# client = chromadb.Client()\n",
    "\n",
    "# current_dir = os.path.dirname(os.path.abspath(__file__))\n",
    "# output_dir = os.path.join(current_dir, 'Index_Store')\n",
    "persisted_directory = \".chromadb/\"\n",
    "client = chromadb.PersistentClient(path=persisted_directory)  # Optional, defaults to .chromadb/ in the current directory\n",
    "persisted_directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3fac2912-f190-4120-b3e6-fdbb62d5a8d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up logging\n",
    "logger = logging.getLogger(__name__)\n",
    "logger.setLevel(logging.INFO)\n",
    "formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')\n",
    "ch = logging.StreamHandler()\n",
    "ch.setLevel(logging.INFO)\n",
    "ch.setFormatter(formatter)\n",
    "logger.addHandler(ch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ed918e0e-c3da-4292-a96a-614f3388440c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install tabulate\n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a340f78b-ef9a-4f7b-af44-ebe0833575ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path = r\"C:\\Users\\123\\Documents\\SD Labs\\Ram Pat\\dataset\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "19de12cf-c816-4cb7-9215-61d6539f0810",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_contents = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "37f9fd95-96c7-4bca-871a-7bcbb50340a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "for file_name in os.listdir(folder_path):\n",
    "    if file_name.endswith(\".txt\"):\n",
    "        file_path = os.path.join(folder_path, file_name)\n",
    "        with open(file_path, \"r\") as file:\n",
    "            content = file.read()\n",
    "            file_contents.append(content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "557f5c37-761d-4217-bf0d-7a86761c8f7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({\"File_Name\": os.listdir(folder_path), \"Content\": file_contents})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1751ecce-c23d-48f3-8ece-b6669d3b4b93",
   "metadata": {},
   "source": [
    "Realize a Análise Exploratória de Dados (EDA) no DataFrame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1f30c8a6-ebea-4fed-9658-00dbb1cb08af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                           File_Name  \\\n",
      "0  part-00000-tid-7978638525664904554-70dd8fb2-21...   \n",
      "1  part-00001-tid-7978638525664904554-70dd8fb2-21...   \n",
      "2  part-00002-tid-7978638525664904554-70dd8fb2-21...   \n",
      "3  part-00003-tid-7978638525664904554-70dd8fb2-21...   \n",
      "4  part-00004-tid-7978638525664904554-70dd8fb2-21...   \n",
      "\n",
      "                                             Content  \n",
      "0  This user with id CCOYNE1 initiated transactio...  \n",
      "1  This user with id KFORD6 initiated transaction...  \n",
      "2  This user with id FMATTING initiated transacti...  \n",
      "3  This user with id CXHUEBNE initiated transacti...  \n",
      "4  This user with id KFORD6 initiated transaction...  \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Ver as primeiras linhas do DataFrame\n",
    "print(df.head())\n",
    "print(\"\\n\")\n",
    "# Informações sobre os tipos de dados e possíveis valores nulos\n",
    "# print(df.info())\n",
    "print(\"\\n\")\n",
    "# Estatísticas descritivas para colunas numéricas (se aplicável)\n",
    "# print(df.describe())\n",
    "print(\"\\n\")\n",
    "# Quantidade de arquivos únicos\n",
    "# print(\"Número de arquivos únicos:\", len(df[\"File_Name\"].unique()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "08282fad-053d-42ff-a667-a079783c7005",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embedding Method with HuggingFace sentence-transformers/all-mpnet-base-v2\n",
    "def huggingFaceEmbedding(document):\n",
    "    logger.info(\"Embedding Method with HuggingFace sentence-transformers/all-mpnet-base-v2\")\n",
    "    # repo_id = \"sentence-transformers/all-mpnet-base-v2\"\n",
    "    repo_id = \"google/flan-t5-xxl\"\n",
    "    embeddings = HuggingFaceHubEmbeddings(\n",
    "        repo_id=repo_id,\n",
    "        task=\"feature-extraction\",\n",
    "        huggingfacehub_api_token=\"HUGGINGFACEHUB_API_TOKEN\",\n",
    "    )\n",
    "\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1ce1a053-3558-4815-a592-2729c49d29f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_files(directory):\n",
    "    documents = []\n",
    "    for filename in os.listdir(directory):\n",
    "        file_path = os.path.join(directory, filename)\n",
    "        file_extension = os.path.splitext(file_path)[-1].lower()\n",
    "\n",
    "        if file_extension == '.txt':\n",
    "            logger.info(f\" it is a {file_extension} file\")\n",
    "            logger.info(f\"Processing txt file: {file_path}\")\n",
    "            # vectorize_txt(file_path)\n",
    "            # load the document and split it into chunks\n",
    "            loader = TextLoader(file_path, encoding='utf-8')\n",
    "            documents = loader.load()\n",
    "            \n",
    "            # split it into chunks\n",
    "            text_splitter = CharacterTextSplitter(separator=\",\",\n",
    "                                          chunk_size=200,\n",
    "                                          chunk_overlap=1)\n",
    "\n",
    "            docs = text_splitter.split_documents(documents)\n",
    "            \n",
    "            # create the open-source embedding function\n",
    "            embedding_function = SentenceTransformerEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "            \n",
    "            # load it into Chroma\n",
    "            db = Chroma.from_documents(docs, embedding_function)\n",
    "            \n",
    "            # query it\n",
    "            # query = \"Give me all users that have anomalies in medication dispensation between Jan and feb of 2023\"\n",
    "            # query = \"Give me all unresolved discrepancies for all users for Area 'A1B1\"\n",
    "            # query = \"Give me the rate of dispensation of medications between the users John doe and PeterParker\"\n",
    "            query = \"Give me all unresolved discrepancies for user John doe (or for fentanyl)\"\n",
    "            # query = \"how many shifts did user JDoe work last week and how many total medications were dispensed? Also give me the anomalies in this data. 5a. Give me reason for the anomalies you gave me above (prompt engineering) Bottom line: the model should do aggregations, filtering, etc. It should also give the results in tabular data.\"\n",
    "\n",
    "\n",
    "            docs = db.similarity_search(query)\n",
    "            \n",
    "            # print results\n",
    "            print(docs[0].page_content)\n",
    "\n",
    "    return documents\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "af74c370-fa37-4afb-9340-e8bb9fbfcfca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_db_disk(persist_directory, embedding, loader):\n",
    "    print(f\"In loading from  disk: {persist_directory} \")\n",
    "    text_splitter = CharacterTextSplitter(separator=\",\",\n",
    "                                          chunk_size=200,\n",
    "                                          chunk_overlap=1)\n",
    "\n",
    "    # text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000,\n",
    "    #                                                chunk_overlap=0)\n",
    "    texts = text_splitter.split_documents(loader)\n",
    "    # texts = text_splitter.create_documents([text])\n",
    "    vectordb = Chroma.from_documents(persist_directory=persist_directory,\n",
    "                                     documents=texts,\n",
    "                                     embedding=embedding)\n",
    "    vectordb.persist()\n",
    "    retriever = VectorStoreRetriever(vectorstore=vectordb)\n",
    "    qa = RetrievalQA.from_chain_type(llm=HuggingFaceHub(),\n",
    "                                     chain_type=\"stuff\",\n",
    "                                     retriever=retriever                                    \n",
    "                                    )\n",
    "    return qa\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8fc0c62a-3e1b-4108-a5e8-52c402ab87e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a different task than the one specified in the repository. Be sure to know what you're doing :)\n"
     ]
    }
   ],
   "source": [
    "# Supplying a persist_directory will store the embeddings on disk\n",
    "embedding = HuggingFaceHubEmbeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4f3c0999-6c2e-46c5-bbf8-d988f1cc4dd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -U sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5fdddb2d-0a48-4974-b2cf-90082e9ede0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# embeddings = HuggingFaceEmbeddings()\n",
    "# text = \"This is a test document.\"\n",
    "# query_result = embeddings.embed_query(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fb7e404-34a3-4cea-b194-a908782b81b6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-01 15:13:03,904 - INFO -  it is a .txt file\n",
      "2023-08-01 15:13:03,905 - INFO - Processing txt file: C:\\Users\\123\\Documents\\SD Labs\\Ram Pat\\dataset\\part-00000-tid-7978638525664904554-70dd8fb2-21ee-44a6-98a5-19edaf12ad15-54461-1-c000.txt\n",
      "2023-08-01 15:14:51,400 - INFO -  it is a .txt file\n",
      "2023-08-01 15:14:51,401 - INFO - Processing txt file: C:\\Users\\123\\Documents\\SD Labs\\Ram Pat\\dataset\\part-00001-tid-7978638525664904554-70dd8fb2-21ee-44a6-98a5-19edaf12ad15-54462-1-c000.txt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "with the date and time 2023-06-12 07:17:13 on item 10964 with name fentaNYL. The transfer type (xty) is 'C'.The quantity issued (qty) is 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-01 15:15:48,983 - INFO -  it is a .txt file\n",
      "2023-08-01 15:15:48,984 - INFO - Processing txt file: C:\\Users\\123\\Documents\\SD Labs\\Ram Pat\\dataset\\part-00002-tid-7978638525664904554-70dd8fb2-21ee-44a6-98a5-19edaf12ad15-54463-1-c000.txt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "with the date and time 2023-06-12 07:17:13 on item 10964 with name fentaNYL. The transfer type (xty) is 'C'.The quantity issued (qty) is 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-01 15:16:44,455 - INFO -  it is a .txt file\n",
      "2023-08-01 15:16:44,456 - INFO - Processing txt file: C:\\Users\\123\\Documents\\SD Labs\\Ram Pat\\dataset\\part-00003-tid-7978638525664904554-70dd8fb2-21ee-44a6-98a5-19edaf12ad15-54464-1-c000.txt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "with the date and time 2023-06-12 07:17:13 on item 10964 with name fentaNYL. The transfer type (xty) is 'C'.The quantity issued (qty) is 1.0\n"
     ]
    }
   ],
   "source": [
    "loader = vectorize_files(folder_path)\n",
    "# global index1\n",
    "# #  We need to clear persisted directory after use\n",
    "# #  or store each document in the different directories\n",
    "index1 = load_db_disk(persisted_directory, embedding, loader)\n",
    "print(f\"Index: {index1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65bd6e96-6aa8-4bcc-aa19-857b346bac07",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask_question(qa, query):\n",
    "    print(f\"qa: {qa}\")\n",
    "    print(f\"Answer question: {query}\")\n",
    "    answer = qa.run(query)\n",
    "    print(f\"answer: {answer}\")\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e799598-ea92-4a04-850e-783482645475",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = 'about what is the text?'\n",
    "ask_question(index1, query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84274971-e3e5-4d7c-bfdc-018d661322e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install ipywidgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e71c5567-5f58-4bbd-b020-d03dd1acae6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install numpy --upgrade --user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbe5de45-94ae-4573-a7d8-9e7953fb2040",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27ddc0c9-6e62-45bd-8a78-7fe07772b18c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5db7dc5d-827f-47cb-bd25-38241196d8a0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
